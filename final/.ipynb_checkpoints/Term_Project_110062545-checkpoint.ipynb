{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "conf = pyspark.SparkConf().setMaster('local').setAppName('lsh')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "def readData(x):\n",
    "    file_id = int(x[0][-7:-4])\n",
    "    content = x[1]\n",
    "    res = []\n",
    "    for l in content.splitlines():\n",
    "\n",
    "        words = l.split(' ')\n",
    "        new_words = []\n",
    "        for i in words:\n",
    "            i = re.sub('[^a-zA-Z0-9]$', '', i)\n",
    "            i = re.sub('^[^a-zA-Z0-9]', '', i)\n",
    "            new_words.append(i)\n",
    "        for i in range(len(new_words)-2):\n",
    "            shingles = new_words[i] + new_words[i+1] + new_words[i+2]\n",
    "            shingles = int.from_bytes(hashlib.sha256(shingles.encode('utf8')).digest()[:4], 'little')\n",
    "            res.append(shingles)\n",
    "\n",
    "    return (file_id, res)\n",
    "\n",
    "def min_hash(x):\n",
    "    l = []\n",
    "    res = []\n",
    "    for i in range(100):\n",
    "        hash_func = i * x[0] % 22111 % keys_conut\n",
    "        l.append(hash_func)\n",
    "    for i in x[1]:\n",
    "        for idx, val in enumerate(l):\n",
    "            res.append(((x[0], i, idx), val))\n",
    "    return res\n",
    "\n",
    "def gen_candidate_pair(x):\n",
    "    res = []\n",
    "    for i in x[1]:\n",
    "        for j in x[1]:\n",
    "            if i != j:\n",
    "                if i < j:\n",
    "                    res.append((i, j))\n",
    "                else:\n",
    "                    res.append((j, i))\n",
    "    return list(set(res))\n",
    "\n",
    "def cal_sim(x):\n",
    "    union = set(x[1][0]).union(set(x[1][1]))\n",
    "    inter = set(x[1][0]).intersection(set(x[1][1]))\n",
    "    \n",
    "    return (x[0], len(inter) / len(union))\n",
    "\n",
    "\n",
    "def hash_bucket(x):\n",
    "    if x[0][0] % 2 == 0:\n",
    "        return ((x[0][0] // 2, x[0][1]), x[1])\n",
    "    else:\n",
    "        return ((x[0][0] // 2, x[0][1]), x[1] * keys_conut)\n",
    "\n",
    "original_data = sc.wholeTextFiles('./athletics/*.txt').map(readData)\n",
    "hash_matrix = original_data.flatMapValues(lambda x: x).map(lambda x: (x[1], [x[0]])).reduceByKey(lambda x, y: x+y).sortBy(lambda x: x[0], ascending=True)\n",
    "hash_matrix = hash_matrix.zipWithIndex().map(lambda x: (x[1], x[0][1]))\n",
    "\n",
    "keys_conut = hash_matrix.keys().count()\n",
    "signature_matrix = hash_matrix.flatMap(min_hash).reduceByKey(lambda x, y: x if x < y else y)\n",
    "\n",
    "lsh_matrix = signature_matrix.map(hash_bucket).reduceByKey(lambda x, y: x + y)\n",
    "lsh_matrix = lsh_matrix.map(lambda x: ((x[0][0], x[1]), [x[0][1]])).reduceByKey(lambda x, y: x + y)\n",
    "candidate_pairs = lsh_matrix.flatMap(gen_candidate_pair).distinct()\n",
    "\n",
    "\n",
    "sim_pairs = candidate_pairs.join(original_data).map(lambda x: (x[1][0], (x[0], x[1][1]))).join(\n",
    "    original_data).map(lambda x: ((x[1][0][0], x[0]), (x[1][0][1], x[1][1])))\n",
    "sim_pairs = sim_pairs.map(cal_sim).sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "ans = sim_pairs.take(10)\n",
    "for i in ans:\n",
    "    print('(%03d, %03d): %.2f %s' % (i[0][0], i[0][1], i[1]*100, '%'))\n",
    "\n",
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
